
# general libraries
import numpy as np
import matplotlib.pyplot as plt

# tensorflow libraries
import tensorflow as tf

# check tensorflow version
print(tf.__version__)

# check eager execution
tf.executing_eagerly()

# SCALAR TENSOR
# This will be an int32 tensor by default; see "dtypes" below.
# A scalar is a tensor without shape (rank is 0 in this case).
# When printing the tensor, you see its value, its shape and its type.
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)

# ARRAY TENSOR
# Let's make this a float tensor.
# An array (a list of elements) is a tensor of rank 1.
rank_1_tensor = tf.constant([2.0, 3.0, 4.0])
print(rank_1_tensor)

# MATRIX TENSOR
# If we want to be specific, we can set the dtype (see below) at creation time.
# A bidimensional matrix is a tensor of rank 2.
rank_2_tensor = tf.constant([[1, 2],
                             [3, 4],
                             [5, 6]], dtype=tf.float16)
print(rank_2_tensor)

# create a variable
a = tf.Variable([2.0, 3.0])
# create another variable b based on the value of a
b = tf.Variable(a)
a.assign([5, 6]) # this command changes a values
# Two variables will not share the same memory.

# a and b are different
print(a.numpy())
print(b.numpy())

# There are other versions of assign
print(a.assign_add([2,3]).numpy())  # [7. 9.]
print(a.assign_sub([7,9]).numpy())  # [0. 0.]

# let's define a constant tensor and print it
a = tf.constant([[1, 2], [3, 4]])
print(a)

# broadcasting is supported
b = tf.add(a, 1)
# numpy is easily integrated
c = np.multiply(a, b)
print(c)

# input tensor (this is a possible example, you can change values for others)
input = tf.constant([[3, 0, 6, 5, 0, 8, 4, 0, 0], 
                     [5, 2, 0, 0, 0, 0, 0, 0, 0], 
                     [0, 8, 7, 0, 0, 0, 0, 3, 1], 
                     [0, 0, 3, 0, 1, 0, 0, 8, 0], 
                     [9, 0, 0, 8, 6, 3, 0, 0, 5], 
                     [0, 5, 0, 0, 9, 0, 6, 0, 0], 
                     [1, 3, 0, 0, 0, 0, 2, 5, 0], 
                     [0, 0, 0, 0, 0, 0, 0, 7, 4], 
                     [0, 0, 5, 2, 0, 6, 3, 0, 0]])

def print_tensor(tensor):
  """Prints the tensor given as input (i.e. the sudoku grid)."""
  print(tensor)

def find_empty_cell(tensor):
  """Find an empty cell inside a tensor, if it exists,
  otherwise returns False."""
  pos0 = tf.where(tensor == 0) # find every 0 present inside tensor
  if len(pos0) != 0: # an empty cell has been found
    return True
  else: # no left empty cells
    return False

def check_validity(tensor, i, j, d):
  """Checks, after assigning the current digit, if the tensor 
  meets constraints or not."""
  # a list of all initial and final indeces of the sub-tensors,
  # to be quickly identified inside the function
  subtensors = [[0,3,0,3],[0,3,3,6],[0,3,6,9],
                [3,6,0,3],[3,6,3,6],[3,6,6,9],
                [6,9,0,3],[6,9,3,6],[6,9,6,9]]
  # check if the current number is already present
  pos_row = tf.where(tensor[i,:] == d) 
  pos_col = tf.where(tensor[:,j] == d) 
  # check for every row and column
  if len(pos_row) != 0 or len(pos_col) != 0:
        return False
  # check for every sub-tensor
  for st in subtensors:
    if i >= st[0] and i < st[1] and j >= st[2] and j < st[3]:
      pos_sub = tf.where(tensor[st[0]:st[1],st[2]:st[3]] == d)
      if len(pos_sub) != 0:
        return False
  return True # all constraints are satisfied!

def generate_elements(tensor):
  """Takes an input tensor and recursively try to insert an element
  and check tensor's validity."""
  tensor_tmp = tf.Variable(tensor)
  # find an empty cell
  if not find_empty_cell(tensor_tmp):
    # if no empty cells are left, you have successfully filled the Sudoku!
    print_tensor(tensor_tmp) 
    return True
  # take the first empty cell and try to fill it
  pos0 = tf.where(tensor_tmp == 0)
  i, j = pos0[0][0], pos0[0][1]
  # try to fill the empty cell with a number from 1 to 9, checking validity
  for d in range(1, 10):
    # check tensor's validity
    if check_validity(tensor_tmp, i, j, d):
      # if all constraints are satisfied, assigned the current element to
      # the current position
      tensor_tmp = tensor_tmp[i, j].assign(d)
      # backtracking (recursion): repeat X times the function itself
      if generate_elements(tensor_tmp):
        return True
      # if constraints are not satisfied (failure), assign a zero to the
      # current position
      tensor_tmp = tensor_tmp[i, j].assign(0)
  return False # continue with backtracking 

generate_elements(input)

# During eager execution, use tf.GradientTape to trace operations 
# for computing gradients later
w = tf.Variable(3.0) # define a tf variable and assign it the value 3
# define a function using tf.GradientTape
with tf.GradientTape() as tape:
  loss = w * w * w 
# calculate gradient with respect to a specific variable (w in this case)
grad = tape.gradient(loss, w)
print(grad)  # => tf.Tensor(27., shape=(), dtype=float32)
