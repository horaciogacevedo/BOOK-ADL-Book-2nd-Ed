
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Linear Regression with One Neuron &#8212; Applied Deep Learning 2nd Edition - Online Companion</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Regression with NumPy" href="Linear_regression_with_numpy.html" />
    <link rel="prev" title="Introduction" href="Introduction.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/TOELT_ADL_2nd.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Applied Deep Learning 2nd Edition - Online Companion</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Get started
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../start/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../start/faq.html">
   Frequently Asked Questions
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapter 11 - An Introduction to TensorFlow 2.x and 1.x
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter11/Computational_graphs_with_TF1.X.html">
   Computational Graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter11/Computational Graphs Exercises.html">
   Computational Graphs - Exercises
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter11/Eager_execution_with_TF2.X.html">
   Eager Execution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter11/Overloading of operators in Tensorflow.html">
   Operator Overloading in TensorFlow
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapter 14 - Single Neuron
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Linear Regression with One Neuron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Linear_regression_with_numpy.html">
   Linear Regression with NumPy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Logistic_regression_with_one_neuron.html">
   Logistic Regression with One Neuron
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapter 25 - Autoencoders
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter25/Your first autoencoder with Keras.html">
   Your first autoencoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter25/Anomaly_detection_with_autoencoders.html">
   Anomaly Detection with Autoencoders
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/Chapter14/Linear_regression_with_one_neuron.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/toelt-llc/ADL-Book-2nd-Ed/master?urlpath=tree/docs/Chapter14/Linear_regression_with_one_neuron.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/toelt-llc/ADL-Book-2nd-Ed/blob/master/docs/Chapter14/Linear_regression_with_one_neuron.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#real-case-example-radon-contamination">
   Real Case Example:
   <strong>
    Radon Contamination
   </strong>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dataset-overview">
     Dataset Overview
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#libraries-and-dataset-import">
     Libraries and Dataset Import
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dataset-splitting">
     Dataset Splitting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-regression-the-model">
     Linear Regression: the Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structure-of-the-net">
     Structure of the Net
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-phase-model-s-learning-phase">
     Training Phase (Model’s Learning Phase)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-phase-model-s-performances-evaluation">
     Testing Phase (Model’s Performances Evaluation)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-readings-a-name-fr-a">
   Further Readings
   <a name="fr">
   </a>
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-regression-with-one-neuron">
<h1>Linear Regression with One Neuron<a class="headerlink" href="#linear-regression-with-one-neuron" title="Permalink to this headline">¶</a></h1>
<p>(C) 2020 - Umberto Michelucci, Michela Sperti</p>
<p>This notebook is part of the book <em>Applied Deep Learning: a case based approach, <strong>2nd edition</strong></em> from APRESS by <a class="reference external" href="mailto:umberto&#46;michelucci&#37;&#52;&#48;toelt&#46;ai">U<span>&#46;</span> Michelucci</a> and <a class="reference external" href="mailto:michela&#46;sperti&#37;&#52;&#48;toelt&#46;ai">M<span>&#46;</span> Sperti</a>.</p>
<p>The purpose of this notebook is to give an example of an application of Linear Regression performed with One Neuron to a dataset taken from real world.</p>
<p><strong>Notebook Learning Goals</strong></p>
<p>At the end of the notebook you are going to have a clear idea of what linear regression is, seen through a practical example. Moreover, you will have learnt what is one of the simplest tasks that Neural Networks can solve. You can of course perform linear regression easily by applying traditional math formulas or using dedicated functions such as those found in scikit-learn. However, it is very instructive to follow this example, since it gives a practical grasp of how the building blocks of Deep Learning architectures (i.e. neurons) work.</p>
<div class="section" id="real-case-example-radon-contamination">
<h2>Real Case Example: <strong>Radon Contamination</strong><a class="headerlink" href="#real-case-example-radon-contamination" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dataset-overview">
<h3>Dataset Overview<a class="headerlink" href="#dataset-overview" title="Permalink to this headline">¶</a></h3>
<p>Radon is a radioactive gas that enters homes through contact points with the ground. It is a carcinogen that is the primary cause of lung cancer in non-smokers. Radon levels vary greatly from household to household. This dataset contains measured radon levels in U.S. homes by county and state. The <em>activity</em> label is the measured radon concentration in <em>pCi/L</em>. Important predictors are:</p>
<ul class="simple">
<li><p><em>floor</em> (the floor of the house in which the measurement was taken),</p></li>
<li><p><em>county</em> (the U.S. county in which the house is located), and</p></li>
<li><p><em>uppm</em> (a measurement of uranium level of the soil by county).</p></li>
</ul>
<p>This dataset fits well a classical regression problem, since it contains a continuous variable (radon activity) which is interesting to be predicted. The model which will be built is made of one neuron and will fit a linear function to floor, county and uppm variables.</p>
</div>
<div class="section" id="libraries-and-dataset-import">
<h3>Libraries and Dataset Import<a class="headerlink" href="#libraries-and-dataset-import" title="Permalink to this headline">¶</a></h3>
<p>This section contains the necessary libraries (such as tensorflow or pandas) you need to import to run the notebook.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># This command install code from the tensorflow docs repository.
# We need to use tensorflow_docs.modeling function when training our model.
# This function will generate a report on the network&#39;s perfomances
# step by step during the training phase (see Training Phase section of the
# notebook). 

# You can safely ignore this cell if you don&#39;t understand what it does.

!pip install git+https://github.com/tensorflow/docs
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting git+https://github.com/tensorflow/docs
  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-0_qrg12m
  Running command git clone -q https://github.com/tensorflow/docs /tmp/pip-req-build-0_qrg12m
Requirement already satisfied (use --upgrade to upgrade): tensorflow-docs===0.0.0146903589e869081cce7006f160331ea5837418c- from git+https://github.com/tensorflow/docs in /usr/local/lib/python3.6/dist-packages
Requirement already satisfied: astor in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0146903589e869081cce7006f160331ea5837418c-) (0.8.1)
Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0146903589e869081cce7006f160331ea5837418c-) (0.10.0)
Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0146903589e869081cce7006f160331ea5837418c-) (3.12.4)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0146903589e869081cce7006f160331ea5837418c-) (3.13)
Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py-&gt;tensorflow-docs===0.0.0146903589e869081cce7006f160331ea5837418c-) (1.15.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf-&gt;tensorflow-docs===0.0.0146903589e869081cce7006f160331ea5837418c-) (50.3.0)
Building wheels for collected packages: tensorflow-docs
  Building wheel for tensorflow-docs (setup.py) ... ?25l?25hdone
  Created wheel for tensorflow-docs: filename=tensorflow_docs-0.0.0146903589e869081cce7006f160331ea5837418c_-cp36-none-any.whl size=143892 sha256=8c20e34b4bfc6e2c07fdb56c9f0d7cf7f4f36f7593610ec657d8b25be210481c
  Stored in directory: /tmp/pip-ephem-wheel-cache-n680b6k2/wheels/eb/1b/35/fce87697be00d2fc63e0b4b395b0d9c7e391a10e98d9a0d97f
Successfully built tensorflow-docs
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># general libraries</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.font_manager</span> <span class="k">as</span> <span class="nn">fm</span>

<span class="c1"># tensorflow libraries</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">import</span> <span class="nn">tensorflow_docs</span> <span class="k">as</span> <span class="nn">tfdocs</span>
<span class="kn">import</span> <span class="nn">tensorflow_docs.modeling</span>

<span class="c1"># ignore warnings</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The following cells are needed to <strong>download</strong> the dataset. You don’t need to understand all the download and processing steps, since the focus of this section is to apply a simple linear regression model to a real case dataset (therefore you can just execute the following cells, ignoring their content). If you are interested in the details, you can find the complete code in the /modules folder.</p>
<p>Now we clone the repository for the book to be able to access the modules that we have written for all the juypter notebooks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Referring to the following cell, if you want to re-clone a repository</span>
<span class="c1"># inside the google colab instance, you need to delete it first. </span>
<span class="c1"># You can delete the repositories contained in this instance executing </span>
<span class="c1"># the following two lines of code (deleting the # comment symbol).</span>

<span class="c1"># !rm -rf ADL-Book-2nd-Ed </span>
<span class="c1"># !rm -rf BCCD_Dataset</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># This command actually clone the repository of the book in the google colab
# instance. In this way this notebook will have access to the modules
# we have written for this book.

# Please note that in case you have already run this cell, and you run it again
# you may get the error message:
#
# fatal: destination path &#39;ADL-Book-2nd-Ed&#39; already exists and is not an empty directory.
# 
# In this case you can safely ignore the error message.

!git clone https://github.com/toelt-llc/ADL-Book-2nd-Ed.git
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fatal: destination path &#39;ADL-Book-2nd-Ed&#39; already exists and is not an empty directory.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># This cell imports some custom written functions that we have created to </span>
<span class="c1"># make the loading of the data and the plotting easier. You don&#39;t need </span>
<span class="c1"># to undertsand the details and you can simply ignore this cell.</span>
<span class="c1"># Simply run it with CMD+Enter (on Mac) or CTRL+Enter (Windows or Ubuntu) to</span>
<span class="c1"># import the necessary functions.</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;ADL-Book-2nd-Ed/modules/&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">read_radon_dataset</span> <span class="kn">import</span> <span class="n">read_data</span>
<span class="kn">from</span> <span class="nn">style_setting</span> <span class="kn">import</span> <span class="n">set_style</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># This cell provides the dataset on which you will implement the linear regression model.</span>
<span class="c1"># Data are temporarily put into &quot;tmp&quot; folder. &quot;url_base&quot; contains the link to access the dataset.</span>

<span class="c1"># After cell&#39;s execution, you will have a variable containing features (&quot;radon_features&quot;),</span>
<span class="c1"># a variable containing labels (&quot;radon_labels&quot;) and an informative variable containing</span>
<span class="c1"># all countries including in the dataset. </span>

<span class="c1"># You don&#39;t need to understand the implementation&#39;s details and you can simply ignore this cell.</span>
<span class="c1"># Simply run it with CMD+Enter (on Mac) or CTRL+Enter (Windows or Ubuntu) to</span>
<span class="c1"># import the necessary functions.</span>

<span class="c1"># inputs to download the dataset</span>
<span class="n">CACHE_DIR</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">sep</span><span class="p">,</span> <span class="s1">&#39;tmp&#39;</span><span class="p">,</span> <span class="s1">&#39;radon&#39;</span><span class="p">)</span>
<span class="n">url_base</span> <span class="o">=</span> <span class="s1">&#39;http://www.stat.columbia.edu/~gelman/arm/examples/radon/&#39;</span>
<span class="c1"># Alternative source:</span>
<span class="c1"># url_base = (&#39;https://raw.githubusercontent.com/pymc-devs/uq_chapter/master/reference/data/&#39;)</span>

<span class="n">rd</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">CACHE_DIR</span><span class="p">,</span> <span class="n">url_base</span><span class="p">)</span>
<span class="n">radon_features</span><span class="p">,</span> <span class="n">radon_labels</span><span class="p">,</span> <span class="n">county_name</span> <span class="o">=</span> <span class="n">rd</span><span class="o">.</span><span class="n">create_dataset</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now you have all the necessary elements to successfully implement this tutorial. <strong>Let’s have a look at our data</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_counties</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">county_name</span><span class="p">)</span>
<span class="n">num_observations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">radon_features</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of counties included in the dataset: &#39;</span><span class="p">,</span> <span class="n">num_counties</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of total samples: &#39;</span><span class="p">,</span> <span class="n">num_observations</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of counties included in the dataset:  85
Number of total samples:  919
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">radon_features</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>floor</th>
      <th>county</th>
      <th>log_uranium_ppm</th>
      <th>pcterr</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>0.502054</td>
      <td>9.7</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0.502054</td>
      <td>14.5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0.502054</td>
      <td>9.6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0.502054</td>
      <td>24.3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>1</td>
      <td>0.428565</td>
      <td>13.8</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The dataset is made of <strong>919 observations</strong>, <strong>1 target</strong> column (<code class="docutils literal notranslate"><span class="pre">activity</span></code>) and <strong>4 features</strong> (<code class="docutils literal notranslate"><span class="pre">floor</span></code>, <code class="docutils literal notranslate"><span class="pre">county</span></code>, <code class="docutils literal notranslate"><span class="pre">log_uranium_ppm</span></code>, <code class="docutils literal notranslate"><span class="pre">pcterr</span></code>). 85 different American counties are included in the dataset.</p>
</div>
<div class="section" id="dataset-splitting">
<h3>Dataset Splitting<a class="headerlink" href="#dataset-splitting" title="Permalink to this headline">¶</a></h3>
<p><em>In any machine learning project, it is a good behaviour to split the dataset you have at your disposal in different subsets.</em> Plenty of theoretical explanations about this need is present in literature. In the <a class="reference external" href="#fr">Further Readings</a> section of the notebook you will find some advice on useful material about this topic. To simply explain the concept: when you build a machine learning model, you first need to train (i.e. build) the model and then you have to test it (i.e. verify the model’s performances on never seen before data). The roughest way to do this is to split the dataset into two subsets: 80% of the original dataset to train the model (the more data you have the better your model will perform) and the remaining 20% to test it.</p>
<p>Now we build a train and a test set splitting the dataset randomly in two parts with the following proportions: <strong>80%/20%</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">radon_features</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.8</span>

<span class="n">train_x</span> <span class="o">=</span> <span class="n">radon_features</span><span class="p">[</span><span class="n">rnd</span><span class="p">]</span> <span class="c1"># training dataset (features)</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">radon_labels</span><span class="p">[</span><span class="n">rnd</span><span class="p">]</span> <span class="c1"># training dataset (labels)</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">radon_features</span><span class="p">[</span><span class="o">~</span><span class="n">rnd</span><span class="p">]</span> <span class="c1"># testing dataset (features)</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">radon_labels</span><span class="p">[</span><span class="o">~</span><span class="n">rnd</span><span class="p">]</span> <span class="c1"># testing dataset (labels)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The training dataset dimensions are: &#39;</span><span class="p">,</span> <span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The testing dataset dimensions are: &#39;</span><span class="p">,</span> <span class="n">test_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The training dataset dimensions are:  (733, 4)
The testing dataset dimensions are:  (186, 4)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="linear-regression-the-model">
<h3>Linear Regression: the Model<a class="headerlink" href="#linear-regression-the-model" title="Permalink to this headline">¶</a></h3>
<p>From now on, the interesting part begins. Keep in mind that a one neuron model is an overkill for a regression task. We could solve linear regression exactly without the need of using gradient descent or similar optimisation algorithm (employed in the neuron’s architecture). You can find an exact regression solution example, implemented with numpy library, in <em>Linear_regression_with_numpy.ipynb</em> notebook.</p>
<p>To note is (for those with some more experience with neural networks) that we will use <strong>one single neuron</strong> with an <strong>identity function</strong> as activation function.</p>
<p>Our dataset can be expressed as a matrix (<span class="math notranslate nohighlight">\(X\)</span>), where the rows represent the different measurements describing radon activity and the columns the 4 features at our disposal. Then, we have the column containing the label we wants to predict (<span class="math notranslate nohighlight">\(y\)</span>). When we employ one neuron to perform linear regression, we are simply computing the following equation:</p>
<p>\begin{equation}
\hat{y}=WX+b
\end{equation}</p>
<p>that is a linear combination of the input data and the network’s weights plus a constat term (bias) <span class="math notranslate nohighlight">\(b\)</span>. <span class="math notranslate nohighlight">\(\hat{y}\)</span> is the predicted output given a certain input vector <span class="math notranslate nohighlight">\({\bf x} = ({\bf x_1}, {\bf x_2}, ... , {\bf x_{n_x}})\)</span>, where we have indicated the number of observations we have at our disposal with <span class="math notranslate nohighlight">\(n_x\)</span>.</p>
</div>
<div class="section" id="structure-of-the-net">
<h3>Structure of the Net<a class="headerlink" href="#structure-of-the-net" title="Permalink to this headline">¶</a></h3>
<p>Despite it is very important to keep in mind the previous considerations, developing this model with Keras is straightforward. The following function builds the one neuron model for linear regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_model</span><span class="p">():</span>

  <span class="c1"># one unit as network&#39;s output</span>
  <span class="c1"># identity function as activation function</span>
  <span class="c1"># sequential groups a linear stack of layers into a tf.keras.Model</span>
  <span class="c1"># activation parameter: if you don&#39;t specify anything, no activation </span>
  <span class="c1"># is applied (i.e. &quot;linear&quot; activation: a(x) = x).</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span> 
    <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">train_x</span><span class="o">.</span><span class="n">columns</span><span class="p">)])</span>
  <span class="p">])</span>

  <span class="c1"># optimizer that implements the RMSprop algorithm</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>

  <span class="c1"># the compile() method takes a metrics argument, which is a list of metrics</span>
  <span class="c1"># loss = Mean Square Error (mse), metrics = Mean Absolute Error (mae),</span>
  <span class="c1"># Mean Square Error (mse)</span>
  <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;mse&#39;</span><span class="p">,</span>
                <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">,</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mse&#39;</span><span class="p">])</span>
  
  <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s have a look at the model summary:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 1)                 5         
=================================================================
Total params: 5
Trainable params: 5
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p><strong>Learning rate</strong> is a very important parameter of the optimizer. In fact, it strongly influences the convergence of the minimization process. It is a common and good behaviour to try different learning rate values and see how the model’s convergence changes. You can find further reading advices about this topic in the <a class="reference external" href="#fr">Further Reading</a> section of this notebook.</p>
</div>
<div class="section" id="training-phase-model-s-learning-phase">
<h3>Training Phase (Model’s Learning Phase)<a class="headerlink" href="#training-phase-model-s-learning-phase" title="Permalink to this headline">¶</a></h3>
<p>Training our neuron means finding the weights and biases that minimize a chosen function (usually called the <strong>cost function</strong> and typically indicated by <span class="math notranslate nohighlight">\(J\)</span>). The cost function to be minimized in the case of a linear regression task is the <strong>mean square error</strong>. The most famous numerical method to find the minimum of a given function is the <strong>gradient descent</strong> (it is suited for cases in which the solution can not be found analytically, such as all neural network applications).</p>
<p>The minimization process based on gradient descent is iterative, therefore it is necessary to decide when to stop it. The simplest way is to set a number of repetitions (called <strong>epochs</strong>) and to run the algorithm that fixed number of times. Then, results are checked to see if an optimal point has been reached. If not, the number of epochs is increased.</p>
<p>We start training our model for <strong>1000</strong> epochs and we look at the summary in terms of performances (MAE and MSE).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
  <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span>
  <span class="n">epochs</span> <span class="o">=</span> <span class="n">EPOCHS</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
  <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">tfdocs</span><span class="o">.</span><span class="n">modeling</span><span class="o">.</span><span class="n">EpochDots</span><span class="p">()])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 0, loss:2543.7136,  mse:2543.7136,  
....................................................................................................
Epoch: 100, loss:19.7100,  mse:19.7100,  
....................................................................................................
Epoch: 200, loss:16.4799,  mse:16.4799,  
....................................................................................................
Epoch: 300, loss:16.0523,  mse:16.0523,  
....................................................................................................
Epoch: 400, loss:16.0160,  mse:16.0160,  
....................................................................................................
Epoch: 500, loss:15.9984,  mse:15.9984,  
....................................................................................................
Epoch: 600, loss:15.9890,  mse:15.9890,  
....................................................................................................
Epoch: 700, loss:15.9610,  mse:15.9610,  
....................................................................................................
Epoch: 800, loss:15.9715,  mse:15.9715,  
....................................................................................................
Epoch: 900, loss:15.9663,  mse:15.9663,  
....................................................................................................
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hist</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
<span class="n">hist</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">epoch</span>
<span class="n">hist</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss</th>
      <th>mse</th>
      <th>epoch</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>995</th>
      <td>15.955948</td>
      <td>15.955948</td>
      <td>995</td>
    </tr>
    <tr>
      <th>996</th>
      <td>15.957788</td>
      <td>15.957788</td>
      <td>996</td>
    </tr>
    <tr>
      <th>997</th>
      <td>15.975211</td>
      <td>15.975211</td>
      <td>997</td>
    </tr>
    <tr>
      <th>998</th>
      <td>15.973335</td>
      <td>15.973335</td>
      <td>998</td>
    </tr>
    <tr>
      <th>999</th>
      <td>15.961553</td>
      <td>15.961553</td>
      <td>999</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>You can noticed that while the number of epochs increases, the MSE is minimized. But <em>which is the best number of epochs to set</em>? A possible hint can be given by the plot of the <strong>cost function vs. number of iterations</strong>. Let’s plot it. If you are interested in plotting details you can find the complete code inside the /module folder.</p>
<p>The cost function vs. number of iterations plot is also useful to evaluate the model’s convergence for different learning rates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The following line contains the path to fonts that are used to plot result in</span>
<span class="c1"># a uniform way.</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">set_style</span><span class="p">()</span><span class="o">.</span><span class="n">set_general_style_parameters</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Cost Function vs. Number of Iterations PLOT</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">],</span> <span class="n">hist</span><span class="p">[</span><span class="s1">&#39;mse&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cost Function (MSE)&#39;</span><span class="p">,</span> <span class="n">fontproperties</span> <span class="o">=</span> <span class="n">fm</span><span class="o">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span> <span class="o">=</span> <span class="n">f</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Iterations&#39;</span><span class="p">,</span> <span class="n">fontproperties</span> <span class="o">=</span> <span class="n">fm</span><span class="o">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span> <span class="o">=</span> <span class="n">f</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">((</span><span class="mi">241</span><span class="o">/</span><span class="mf">255.0</span><span class="p">,</span> <span class="mi">247</span><span class="o">/</span><span class="mf">255.0</span><span class="p">,</span> <span class="mi">240</span><span class="o">/</span><span class="mf">255.0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Linear_regression_with_one_neuron_42_0.png" src="../_images/Linear_regression_with_one_neuron_42_0.png" />
</div>
</div>
<p>Looking at the previous plot, you can notice that, after 400 epochs, the cost function remains almost constant in its value, indicating that a minimum has been reached.</p>
<p>Let’s have a look at the <strong>estimated weights</strong> of our neuron model. The first four can be seen as the linear regression coefficients, while the last one is the bias term. Keep in mind that the neuron performs the following operation:</p>
<p>\begin{equation}
\hat{y}=WX+b
\end{equation}</p>
<p>You can compare these numbers with the ones obtained in the <em>Linear_regression_with_numpy.ipynb</em> notebook.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span> <span class="c1"># return a numpy list of weights</span>
<span class="nb">print</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[array([[-6.6795307e-01],
       [ 2.7279984e-03],
       [ 2.8733387e+00],
       [-2.0828046e-01]], dtype=float32), array([4.2394686], dtype=float32)]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="testing-phase-model-s-performances-evaluation">
<h3>Testing Phase (Model’s Performances Evaluation)<a class="headerlink" href="#testing-phase-model-s-performances-evaluation" title="Permalink to this headline">¶</a></h3>
<p>Now, to know if the model you have just built is suited to be applied to unseen data, you have to check its performances over the test set. In the following cell the linear model is applied to the test set to make predictions (<code class="docutils literal notranslate"><span class="pre">test_predictions</span></code>). Then, predicted radon activity values are compared with real values (<code class="docutils literal notranslate"><span class="pre">test_y</span></code>) by simply plotting <strong>Predictions vs. True Values</strong>. An optimal model shows points distributed over the black solid line present in the plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_x</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="c1"># predict radon activities with the built linear regression model</span>

<span class="c1"># Predictions vs. True Values PLOT</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">test_y</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">ls</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Predictions [activity]&#39;</span><span class="p">,</span> <span class="n">fontproperties</span> <span class="o">=</span> <span class="n">fm</span><span class="o">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span> <span class="o">=</span> <span class="n">f</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;True Values [activity]&#39;</span><span class="p">,</span> <span class="n">fontproperties</span> <span class="o">=</span> <span class="n">fm</span><span class="o">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span> <span class="o">=</span> <span class="n">f</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Linear Regression with One Neuron&#39;</span><span class="p">,</span> <span class="n">fontproperties</span> <span class="o">=</span> <span class="n">fm</span><span class="o">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span> <span class="o">=</span> <span class="n">f</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">((</span><span class="mi">241</span><span class="o">/</span><span class="mf">255.0</span><span class="p">,</span> <span class="mi">247</span><span class="o">/</span><span class="mf">255.0</span><span class="p">,</span> <span class="mi">240</span><span class="o">/</span><span class="mf">255.0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Linear_regression_with_one_neuron_48_0.png" src="../_images/Linear_regression_with_one_neuron_48_0.png" />
</div>
</div>
<p>This plots shows that linear regression is too simple to accurately model this dataset’s behaviour and a more complex model is needed.</p>
</div>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>[<em>Easy Difficulty</em>] Try using only one feature to predict radon activity and see how results change.</p></li>
<li><p>[<em>Medium Difficulty</em>] Try to change the <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> parameter and see how the model’s convergence changes. Then try to reduce the <code class="docutils literal notranslate"><span class="pre">EPOCHS</span></code> parameter and see when the model cannot reach convergence.</p></li>
<li><p>[<em>Medium Difficulty</em>] Try to see how model’s results change based on the training dataset’s size (reduce it and use different sizes comparing the final results).</p></li>
</ol>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>https://www.tensorflow.org/probability/examples/Multilevel_Modeling_Primer (radon dataset loading and preprocessing)</p></li>
<li><p>https://www.tensorflow.org/datasets/catalog/radon (dataset explanation)</p></li>
</ol>
</div>
<div class="section" id="further-readings-a-name-fr-a">
<h2>Further Readings <a name = "fr"></a><a class="headerlink" href="#further-readings-a-name-fr-a" title="Permalink to this headline">¶</a></h2>
<p><strong>Dataset Splitting, Overfitting &amp; Underfitting</strong></p>
<ol class="simple">
<li><p>Lever, Jake, Martin Krzywinski, and Naomi Altman. “Points of significance: model selection and overfitting.” (2016): 703.</p></li>
<li><p>Srivastava, Nitish, et al. “Dropout: a simple way to prevent neural networks from overfitting.” The journal of machine learning research 15.1 (2014): 1929-1958.</p></li>
</ol>
<p><strong>Learning Rate</strong></p>
<ol class="simple">
<li><p>Bengio, Yoshua. “Practical recommendations for gradient-based training of deep architectures.” Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 437-478.</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Chapter14"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="Introduction.html" title="previous page">Introduction</a>
    <a class='right-next' id="next-link" href="Linear_regression_with_numpy.html" title="next page">Linear Regression with NumPy</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Umberto Michelucci and Michela Sperti<br/>
        
            &copy; Copyright TOELT LLC (2020-2021).<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>