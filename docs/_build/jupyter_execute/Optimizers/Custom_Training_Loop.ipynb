{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6hyi7Kvrs75"
   },
   "source": [
    "# Simple example of a custom training loop\n",
    "\n",
    "Version 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOzjCchZsCK9"
   },
   "source": [
    "(C) 2020 - Umberto Michelucci, Michela Sperti\n",
    "\n",
    "This notebook is part of the book _Applied Deep Learning: a case based approach, **2nd edition**_ from APRESS by [U. Michelucci](mailto:umberto.michelucci@toelt.ai) and [M. Sperti](mailto:michela.sperti@toelt.ai).\n",
    "\n",
    "The goal of this notebook is to show how a custom training loop looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3004,
     "status": "ok",
     "timestamp": 1643390245644,
     "user": {
      "displayName": "Dr. Umberto Michelucci",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggrkq7MAw6QQDY8sxIXNAo8wGBCTJXUym30fmVK=s64",
      "userId": "10493425851464515093"
     },
     "user_tz": -60
    },
    "id": "01aaAHv1CTFV",
    "outputId": "9e005209-166b-4813-956b-38949092723a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STTNusFzsS9c"
   },
   "source": [
    "A custom training loop is based on the fundamental capacity of Keras to evaluate gradients and derivatives automatically without you doing any math. Remember that training a network with backpropagation means calculating the gradients of the loss function.\n",
    "\n",
    "In this example let's start to see how to evaluate the derivative of the function\n",
    "\n",
    "$$\n",
    "y=x^2\n",
    "$$\n",
    "\n",
    "and let's try to evaluate it at $x=3$. If you know calculus you should see quickly that\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx}(3) = 2x|_{x=3}=6\n",
    "$$\n",
    "\n",
    "This can easily done by using the ```GradientTape()``` context (https://www.tensorflow.org/api_docs/python/tf/GradientTape) as you can see in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1643390466634,
     "user": {
      "displayName": "Dr. Umberto Michelucci",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggrkq7MAw6QQDY8sxIXNAo8wGBCTJXUym30fmVK=s64",
      "userId": "10493425851464515093"
     },
     "user_tz": -60
    },
    "id": "OUlwS85fEV1E",
    "outputId": "1c25ff5e-0267-45f3-da6c-dc7ae904e7ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_8U5nw2tRGk"
   },
   "source": [
    "The same approach work exactly the same when you are dealing with a neural network. In the example below you can see how to calculate the gradient of the loss function of a simple neural network with just one dense layer with 2 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1643390547786,
     "user": {
      "displayName": "Dr. Umberto Michelucci",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggrkq7MAw6QQDY8sxIXNAo8wGBCTJXUym30fmVK=s64",
      "userId": "10493425851464515093"
     },
     "user_tz": -60
    },
    "id": "3bK75CdjFsay"
   },
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # Forward pass\n",
    "  y = layer(x)\n",
    "  loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCXfI9-xtfOQ"
   },
   "source": [
    "In the cell below you can see how to retrieve the value of the trainable parameters and their gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1643390586146,
     "user": {
      "displayName": "Dr. Umberto Michelucci",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggrkq7MAw6QQDY8sxIXNAo8wGBCTJXUym30fmVK=s64",
      "userId": "10493425851464515093"
     },
     "user_tz": -60
    },
    "id": "Dh98RHgKFtUF",
    "outputId": "3d050270-0054-4c05-d00a-725f3c9c72c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0, shape: (3, 2)\n",
      "dense/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "  print(f'{var.name}, shape: {g.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W02dlRjPt0Iq"
   },
   "source": [
    "## Custom training loop for a neural network with MNIST data\n",
    "\n",
    "Now let's use this approach to a real network. Let's consider a network with two dense layers, each having 64 neurons, used to classify MNIST images. If you don't know what MNIST is check [THIS LINK](http://yann.lecun.com/exdb/mnist/). TL;DR MNIST is a dataset composed of 70000 28x28 gray level images of digits. There are roughly 7000 images for each of each digit (0 to 9).\n",
    "\n",
    "First things first: let's define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1643390819605,
     "user": {
      "displayName": "Dr. Umberto Michelucci",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggrkq7MAw6QQDY8sxIXNAo8wGBCTJXUym30fmVK=s64",
      "userId": "10493425851464515093"
     },
     "user_tz": -60
    },
    "id": "PQJkpmPLCaAx"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
    "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1643390825335,
     "user": {
      "displayName": "Dr. Umberto Michelucci",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggrkq7MAw6QQDY8sxIXNAo8wGBCTJXUym30fmVK=s64",
      "userId": "10493425851464515093"
     },
     "user_tz": -60
    },
    "id": "sfOvZ-shueJl",
    "outputId": "7554de55-b0e3-429b-9c04-ca9d05d8e3cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " digits (InputLayer)         [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eFmmXOSumfh"
   },
   "source": [
    "Secondly we need the typical components of a netwok model: an optimizer, the loss function and the dataset. Nothing special to see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1643391244038,
     "user": {
      "displayName": "Dr. Umberto Michelucci",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggrkq7MAw6QQDY8sxIXNAo8wGBCTJXUym30fmVK=s64",
      "userId": "10493425851464515093"
     },
     "user_tz": -60
    },
    "id": "kT3Mbcl1CdPY"
   },
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "# \n",
    "# Note that when developing custom training loop you cannot \n",
    "# use model.evluate() therefore you need to track the metrics\n",
    "# manually.\n",
    "#\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiCMZEFpuwJ8"
   },
   "source": [
    "Now finally let's train our small network with a custom training loop. Check the code and the comments and you should immediately see what each component is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20878,
     "status": "ok",
     "timestamp": 1643391398536,
     "user": {
      "displayName": "Dr. Umberto Michelucci",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggrkq7MAw6QQDY8sxIXNAo8wGBCTJXUym30fmVK=s64",
      "userId": "10493425851464515093"
     },
     "user_tz": -60
    },
    "id": "_3gXJVC4ChHX",
    "outputId": "7dad6d29-5534-434c-f681-661506e2b197"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 0.5484\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.3190\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.2064\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.2227\n",
      "Seen so far: 38464 samples\n",
      "Training loss (for one batch) at step 800: 0.3685\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.8809\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.1432\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 0.2973\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 0.2833\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 0.3132\n",
      "Seen so far: 38464 samples\n",
      "Training loss (for one batch) at step 800: 0.4082\n",
      "Seen so far: 51264 samples\n",
      "Training acc over epoch: 0.9048\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 64))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYH2uJwWvt_1"
   },
   "source": [
    "At the end of each epoch you can see the training accuracy that we have decided to track. This short notebook should have given an idea on how to implement a custom training loop with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGt8AakQwy3b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOxU/Hg8zUhh1jGSNWSaMvV",
   "collapsed_sections": [],
   "name": "Custom_Training Loop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}