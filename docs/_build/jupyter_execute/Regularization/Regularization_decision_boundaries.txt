# general libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# tensorflow libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# sklearn libraries
from sklearn import linear_model

nobs = 30 # number of observations
np.random.seed(42) # making results reproducible

# first set of observations
xx1 = np.array([np.random.normal(0.3, 0.15) for i in range (0, nobs)])
yy1 = np.array([np.random.normal(0.3, 0.15) for i in range (0, nobs)])
# second set of observations
xx2 = np.array([np.random.normal(0.1, 0.1) for i in range (0, nobs)])
yy2 = np.array([np.random.normal(0.3, 0.1) for i in range (0, nobs)])
# concatenating observations
c1_ = np.c_[xx1.ravel(), yy1.ravel()]
c2_ = np.c_[xx2.ravel(), yy2.ravel()] 
c = np.concatenate([c1_, c2_])
# creating the labels
yy1_ = np.full(nobs, 0, dtype = int)
yy2_ = np.full(nobs, 1, dtype = int)
yyL = np.concatenate((yy1_, yy2_), axis = 0)
# defining training points and labels
train_x = c
train_y = yyL

def create_and_train_regularized_model(data_train_norm, labels_train, num_neurons, num_layers, n_epochs, lambda_):
    # build model
    inputs = keras.Input(shape = data_train_norm.shape[1]) # input layer
    # he initialization
    initializer = tf.keras.initializers.HeNormal()
    # regularization
    reg = tf.keras.regularizers.l2(l2 = lambda_)
    # first hidden layer
    dense = layers.Dense(num_neurons, activation = 'relu', kernel_initializer = initializer, kernel_regularizer = reg)(inputs)
    # customized number of layers and neurons per layer
    for i in range(num_layers - 1):       
        dense = layers.Dense(num_neurons, activation = 'relu', kernel_initializer = initializer, kernel_regularizer = reg)(dense)
    # output layer   
    outputs = layers.Dense(1, activation = 'sigmoid')(dense)
    model = keras.Model(inputs = inputs, outputs = outputs, name = 'model')
    # set optimizer and loss
    opt = keras.optimizers.Adam(learning_rate = 0.005)
    model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'])
    
    # train model
    history = model.fit(
      data_train_norm, labels_train,
      epochs = n_epochs, verbose = 0,
      batch_size = data_train_norm.shape[0])
    # save performances
    hist = pd.DataFrame(history.history)
    hist['epoch'] = history.epoch 

    return hist, model

hist, model = create_and_train_regularized_model(train_x, train_y, 20, 4, 100, 0.0)

def make_mesh_predict(h, c1, clf):
    """
    Inputs:
    h -> mesh step (0.001 is a good value if you move between 0 and 1)
    c1 -> your training data
    clf -> your model
    
    Outputs:
    xx -> x values of the mesh
    yy -> y values of the mesh
    Z -> the prediction (the color of each point)
    """   
    # point in the mesh [x_min, x_max] x [y_min, y_max].
    x_min, x_max = c1[:, 0].min() - .2, c1[:, 0].max() + .2
    y_min, y_max = c1[:, 1].min() - .2, c1[:, 1].max() + .2
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    
    Z = Z.reshape(xx.shape)
    
    return xx, yy, Z

def print_decision_boundary(xx, yy, Z, X, Y):
    """
    Inputs:
    xx: the values for the mesh (coming from make_mesh_predict())
    yy: the values for the mesh (coming from make_mesh_predict())
    Z: the prediction for each point (that will be mapped to the color)
    X,Y: the training points
    """
    fig = plt.figure(figsize = (9, 7))
    ax = fig.add_subplot(111)
    plt.pcolormesh(xx, yy, Z, cmap = 'Greys', alpha = 0.1)

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c = Y * 100, edgecolors = 'k',  s = 40, cmap = 'gray')
    plt.xlabel('$x$')
    plt.ylabel('$y$')

    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.xticks(())
    plt.yticks(())
    
    #fig.savefig('Figure16-7.png', dpi = 300)
    plt.show()

xx, yy, Z = make_mesh_predict(0.001, train_x, model)

print_decision_boundary(xx, yy, Z, train_x, train_y)

hist_reg, model_reg = create_and_train_regularized_model(train_x, train_y, 20, 4, 100, 0.04)

xx, yy, Z = make_mesh_predict(0.001, train_x, model_reg)

print_decision_boundary(xx, yy, Z, train_x, train_y)

logreg = linear_model.LogisticRegression()
logreg.fit(train_x, train_y)

xx1, yy1, Z1 = make_mesh_predict(0.001, train_x, logreg)

print_decision_boundary(xx1, yy1, Z1, train_x, train_y)

fig = plt.figure(figsize = (9, 7))
plt.pcolormesh(xx1, yy1, Z1, cmap = 'Greys', alpha = 0.02)
plt.pcolormesh(xx, yy, Z, cmap = 'Greys', alpha = 0.1)

# Plot also the training points
plt.scatter(train_x[:, 0], train_x[:, 1], c = train_y * 100, edgecolors = 'k',  s = 40, cmap = 'gray') 
plt.xlabel('$x$')
plt.ylabel('$y$')

plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.xticks(())
plt.yticks(())

#fig.savefig('Figure16-8.png', dpi = 300)
plt.show()
