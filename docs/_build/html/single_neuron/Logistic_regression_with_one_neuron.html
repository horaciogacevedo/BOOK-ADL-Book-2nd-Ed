
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Logistic Regression with One Neuron &#8212; Applied Deep Learning - 2nd Edition</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Multiclass Classification with Fully Connected Networks" href="../FFNN/Multiclass_classification_with_fully_connected_networks.html" />
    <link rel="prev" title="Linear Regression with NumPy" href="Linear_regression_with_numpy.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo_book_online.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Applied Deep Learning - 2nd Edition</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../landingpage.html">
   Neural Networks and deep learning: theory and applications
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  Get started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../start/introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../start/faq.html">
   Frequently Asked Questions
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  An Introduction to TensorFlow 2.x and 1.x
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Introduction/Computational_graphs_with_TF1.X.html">
   Computational Graphs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Introduction/Computational_graphs_exercises.html">
   Computational Graphs - Exercises
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Introduction/Eager_execution_with_TF2.X.html">
   Eager Execution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Introduction/Overloading_of_operators_in_TF.html">
   Operator Overloading in TensorFlow
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  I - Single Neuron
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Linear_regression_with_one_neuron.html">
   Linear Regression with One Neuron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Linear_regression_with_numpy.html">
   Linear Regression with NumPy
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Logistic Regression with One Neuron
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  II - Fully Connected Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../FFNN/Multiclass_classification_with_fully_connected_networks.html">
   Multiclass Classification with Fully Connected Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../FFNN/Overfitting_example.html">
   Overfitting Example
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  III - Regularization Techniques
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Regularization/Regularization_techniques.html">
   Regularization Techniques
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Regularization/Regularization_decision_boundaries.html">
   Regularization Decision Boundaries
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  IV - Gradient Descent Optimizer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gradientDescent/Gradient_descent_developed_from_scratch.html">
   Gradient Descent
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  V - Advanced Optimizers
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Optimizers/Custom_Training_Loop.html">
   Simple example of a custom training loop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Optimizers/Optimizers_comparison.html">
   Comparison of Optimizers
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  VI - Hyper-parameter Tuning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../hyper/Hyperparameter_tuning_with_Zalando_dataset.html">
   Hyperparameter Tuning with the Zalando Dataset
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  VII - Convolutional Neural Networks (CNN)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../CNN/The_convolution_operator.html">
   The Convolution Operator
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../CNN/Multiclass_classification_with_convolutional_neural_networks.html">
   Multiclass Classification with Convolutional Neural Networks
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  VIII - Recurrent Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../RNN/Counting_with_recurrent_neural_networks.html">
   Counting with Recurrent Neural Networks
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  IX - Metric Analysis
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../metric/Metric_analysis_with_MNIST_dataset.html">
   Metric Analysis
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  IX - Generative Adversarial Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../GAN/GAN_with_MNIST.html">
   GAN Example in TensorFlow
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  X - Autoencoders
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Autoencoders/Your_first_autoencoder_with_Keras.html">
   Your First Autoencoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Autoencoders/Anomaly_detection_with_autoencoders.html">
   Anomaly Detection with Autoencoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Autoencoders/Denoising_autoencoders_with_FFNN.html">
   Denoising Images with Autoencoders based on Feed-Forward Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Autoencoders/Denoising_autoencoders_with_CNN.html">
   Denoising Images with Autoencoders based on Convolutional Neural Networks
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  XI - Variational Autoencoders
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../VAE/Variational_Autoencoders.html">
   Your first variational autoencoder
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/single_neuron/Logistic_regression_with_one_neuron.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/toelt-llc/ADL-Book-2nd-Ed"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/toelt-llc/ADL-Book-2nd-Ed/master?urlpath=tree/docs/single_neuron/Logistic_regression_with_one_neuron.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/toelt-llc/ADL-Book-2nd-Ed/blob/master/docs/single_neuron/Logistic_regression_with_one_neuron.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#notebook-learning-goals">
   Notebook Learning Goals
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#real-case-example-blood-cells-detection">
   Real Case Example:
   <strong>
    Blood Cells Detection
   </strong>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dataset-overview">
     Dataset Overview
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#libraries-and-dataset-import">
     Libraries and Dataset Import
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dataset-splitting">
     Dataset Splitting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#logistic-regression-the-model">
     Logistic Regression: the Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structure-of-the-net">
     Structure of the Net
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training-phase-model-s-learning-phase">
     Training Phase (Model’s Learning Phase)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing-phase-model-s-performances-evaluation">
     Testing Phase (Model’s Performances Evaluation)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#exercises">
   Exercises
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-readings-a-name-fr-a">
   Further Readings
   <a name="fr">
   </a>
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="logistic-regression-with-one-neuron">
<h1>Logistic Regression with One Neuron<a class="headerlink" href="#logistic-regression-with-one-neuron" title="Permalink to this headline">¶</a></h1>
<p>Version 1.05</p>
<p>(C) 2020 - Umberto Michelucci, Michela Sperti</p>
<p>This notebook is part of the book <em>Applied Deep Learning: a case based approach, <strong>2nd edition</strong></em> from APRESS by <a class="reference external" href="mailto:umberto&#46;michelucci&#37;&#52;&#48;toelt&#46;ai">U<span>&#46;</span> Michelucci</a> and <a class="reference external" href="mailto:michela&#46;sperti&#37;&#52;&#48;toelt&#46;ai">M<span>&#46;</span> Sperti</a>.</p>
<p>The purpose of this notebook is to give an example of an application of Logistic Regression performed with One Neuron to a dataset taken from real world.</p>
<div class="section" id="notebook-learning-goals">
<h2>Notebook Learning Goals<a class="headerlink" href="#notebook-learning-goals" title="Permalink to this headline">¶</a></h2>
<p>At the end of the notebook you are going to have a clear idea of what logistic regression is, seen through a practical example. It is very instructive to compare this notebook with <em>Linear_Regression_with_one_neuron.ipynb</em> one, since they both are applications of the One Neuron model, used to solve different problems. Along this notebook you will be provided with information about this model to notice similarities and differences with the Linear Regression one. You are going to see how simple is using Keras and how, changing a few parameters, you can easily obtain a different model that can solve a different problem.</p>
</div>
<div class="section" id="real-case-example-blood-cells-detection">
<h2>Real Case Example: <strong>Blood Cells Detection</strong><a class="headerlink" href="#real-case-example-blood-cells-detection" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dataset-overview">
<h3>Dataset Overview<a class="headerlink" href="#dataset-overview" title="Permalink to this headline">¶</a></h3>
<p>In this notebook we will use the BCCD Dataset, a small-scale dataset for blood cells detection. The dataset will be downloaded from its GitHub repository.</p>
<p>From this dataset, nicolaschen1 developed two Python scripts to make preparation data for recognition of abnormalities in blood cells on medical images. In the notebook, a slightly modified version of the two scripts will be used.</p>
<ol class="simple">
<li><p>a script to create the pandas dataframe with all data needed: <em>filename</em>, <em>cell_type</em>, <em>xmin</em>, <em>xmax</em>, <em>ymin</em>, <em>ymax</em>,</p></li>
<li><p>a script to plot the boxes for each image and save it in a new directory.</p></li>
</ol>
<p>The Image Type is jpeg(JPEG) with Width x Height: 640 x 480.</p>
<p>The dataset contains three kind of labels::</p>
<ol class="simple">
<li><p><em>RBC</em> (Red Blood Cell)</p></li>
<li><p><em>WBC</em> (White Blood Cell)</p></li>
<li><p><em>Platelets</em></p></li>
</ol>
<p>To keep it simple, we will consider only RBC and WBC to be predicted. In detail, we will face a typical classification problem. The model which will be built is made of one neuron and will predict if an image contains RBC or WBC from <code class="docutils literal notranslate"><span class="pre">xmin</span></code>, <code class="docutils literal notranslate"><span class="pre">xmax</span></code>, <code class="docutils literal notranslate"><span class="pre">ymin</span></code> and <code class="docutils literal notranslate"><span class="pre">ymax</span></code> variables.</p>
</div>
<div class="section" id="libraries-and-dataset-import">
<h3>Libraries and Dataset Import<a class="headerlink" href="#libraries-and-dataset-import" title="Permalink to this headline">¶</a></h3>
<p>This section contains the necessary libraries (such as tensorflow or pandas) you need to import to run the notebook.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># This command install code from the tensorflow docs repository.
# We need to use tensorflow_docs.modeling function when training our model.
# This function will generate a report on the network&#39;s perfomances
# step by step during the training phase (see Training Phase section of the
# notebook). 

# You can safely ignore this cell if you don&#39;t understand what it does.

!pip install git+https://github.com/tensorflow/docs
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Collecting git+https://github.com/tensorflow/docs
  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-u6l28m4v
  Running command git clone -q https://github.com/tensorflow/docs /tmp/pip-req-build-u6l28m4v
Requirement already satisfied (use --upgrade to upgrade): tensorflow-docs===0.0.0d7cf3b307cf13e5aadea359db22a77a1cb04f499- from git+https://github.com/tensorflow/docs in /usr/local/lib/python3.6/dist-packages
Requirement already satisfied: astor in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0d7cf3b307cf13e5aadea359db22a77a1cb04f499-) (0.8.1)
Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0d7cf3b307cf13e5aadea359db22a77a1cb04f499-) (0.10.0)
Requirement already satisfied: protobuf&gt;=3.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0d7cf3b307cf13e5aadea359db22a77a1cb04f499-) (3.14.0)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0d7cf3b307cf13e5aadea359db22a77a1cb04f499-) (3.13)
Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tensorflow-docs===0.0.0d7cf3b307cf13e5aadea359db22a77a1cb04f499-) (0.8)
Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py-&gt;tensorflow-docs===0.0.0d7cf3b307cf13e5aadea359db22a77a1cb04f499-) (1.15.0)
Building wheels for collected packages: tensorflow-docs
  Building wheel for tensorflow-docs (setup.py) ... ?25l?25hdone
  Created wheel for tensorflow-docs: filename=tensorflow_docs-0.0.0d7cf3b307cf13e5aadea359db22a77a1cb04f499_-cp36-none-any.whl size=146696 sha256=408592b6da363168157ef889afa05405fcbef6e2e74a54a4acd891cb5ceebac4
  Stored in directory: /tmp/pip-ephem-wheel-cache-6e3wnocm/wheels/eb/1b/35/fce87697be00d2fc63e0b4b395b0d9c7e391a10e98d9a0d97f
Successfully built tensorflow-docs
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># general libraries</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.font_manager</span> <span class="k">as</span> <span class="nn">fm</span>

<span class="c1"># tensorflow libraries</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">import</span> <span class="nn">tensorflow_docs</span> <span class="k">as</span> <span class="nn">tfdocs</span>
<span class="kn">import</span> <span class="nn">tensorflow_docs.modeling</span>

<span class="c1"># ignore warnings</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The following cells are needed to <strong>download</strong> the dataset. You don’t need to understand all the download and processing steps, since the focus of this section is to apply a logistic regression model to a real case dataset (therefore you can just execute the following cells, ignoring their content). If you are interested in the details, you can find the complete code in the /modules folder.</p>
<p>Now we clone the repository for the book, to be able to access the modules that we have written for all the juypter notebooks, and the repository which contains the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Referring to the following cell, if you want to re-clone a repository</span>
<span class="c1"># inside the google colab instance, you need to delete it first. </span>
<span class="c1"># You can delete the repositories contained in this instance executing </span>
<span class="c1"># the following two lines of code (deleting the # comment symbol).</span>

<span class="c1"># !rm -rf ADL-Book-2nd-Ed </span>
<span class="c1"># !rm -rf BCCD_Dataset</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># This command actually clone the repository of the book in the google colab
# instance. In this way this notebook will have access to the modules
# we have written for this book.

# Please note that in case you have already run this cell, and you run it again
# you may get the error message:
#
# fatal: destination path &#39;ADL-Book-2nd-Ed&#39; already exists and is not an empty directory.
# 
# In this case you can safely ignore the error message.
!git clone https://github.com/toelt-llc/ADL-Book-2nd-Ed.git
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fatal: destination path &#39;ADL-Book-2nd-Ed&#39; already exists and is not an empty directory.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># We also clone the repository containing the dataset. 

!git clone https://github.com/Shenggan/BCCD_Dataset.git
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fatal: destination path &#39;BCCD_Dataset&#39; already exists and is not an empty directory.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># This cell imports some custom written functions that we have created to </span>
<span class="c1"># make the loading of the data and the plotting easier. You don&#39;t need </span>
<span class="c1"># to undertsand the details and you can simply ignore this cell.</span>
<span class="c1"># Simply run it with CMD+Enter (on Mac) or CTRL+Enter (Windows or Ubuntu) to</span>
<span class="c1"># import the necessary functions.</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;ADL-Book-2nd-Ed/modules/&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">read_bccd_dataset</span> <span class="kn">import</span> <span class="n">read_data</span>
<span class="kn">from</span> <span class="nn">style_setting</span> <span class="kn">import</span> <span class="n">set_style</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># This cell provides the dataset on which you will implement the logistic regression model.</span>

<span class="c1"># After cell&#39;s execution, you will have a pandas dataframe containing filenames,</span>
<span class="c1"># features (xmin, xmax, ymin, ymax), and labels (cell_type).</span>

<span class="c1"># You don&#39;t need to understand the implementation&#39;s details and you can simply ignore this cell.</span>
<span class="c1"># Simply run it with CMD+Enter (on Mac) or CTRL+Enter (Windows or Ubuntu) to</span>
<span class="c1"># import the necessary functions.</span>

<span class="n">rd</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">()</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">rd</span><span class="o">.</span><span class="n">preprocess_bccd_dataset</span><span class="p">()</span>
<span class="n">dataset_reduced</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;cell_type&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;RBC&#39;</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;cell_type&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;WBC&#39;</span><span class="p">)]</span>
<span class="n">bccd_features</span> <span class="o">=</span> <span class="n">dataset_reduced</span><span class="p">[[</span><span class="s1">&#39;xmin&#39;</span><span class="p">,</span> <span class="s1">&#39;xmax&#39;</span><span class="p">,</span> <span class="s1">&#39;ymin&#39;</span><span class="p">,</span> <span class="s1">&#39;ymax&#39;</span><span class="p">]]</span>
<span class="n">bccd_labels</span> <span class="o">=</span> <span class="n">dataset_reduced</span><span class="p">[</span><span class="s1">&#39;cell_type&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now you have all the necessary elements to successfully implement this tutorial. <strong>Let’s have a look at our data</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_observations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">bccd_features</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of total samples: &#39;</span><span class="p">,</span> <span class="n">num_observations</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of total samples:  4527
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bccd_features</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>xmin</th>
      <th>xmax</th>
      <th>ymin</th>
      <th>ymax</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>349</td>
      <td>499</td>
      <td>120</td>
      <td>206</td>
    </tr>
    <tr>
      <th>1</th>
      <td>343</td>
      <td>437</td>
      <td>272</td>
      <td>357</td>
    </tr>
    <tr>
      <th>2</th>
      <td>184</td>
      <td>283</td>
      <td>292</td>
      <td>378</td>
    </tr>
    <tr>
      <th>3</th>
      <td>253</td>
      <td>352</td>
      <td>240</td>
      <td>326</td>
    </tr>
    <tr>
      <th>4</th>
      <td>408</td>
      <td>505</td>
      <td>282</td>
      <td>393</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The dataset is made of <strong>4527 observations</strong>, <strong>1 target</strong> column (<code class="docutils literal notranslate"><span class="pre">cell_type</span></code>) and <strong>4 features</strong> (<code class="docutils literal notranslate"><span class="pre">xmin</span></code>, <code class="docutils literal notranslate"><span class="pre">xmax</span></code>, <code class="docutils literal notranslate"><span class="pre">ymin</span></code>, <code class="docutils literal notranslate"><span class="pre">ymax</span></code>).</p>
<p>When working with images, it is useful to get an idea of how they look. Let’s plot an example image from our dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The following line contains the path to fonts that are used to plot result in</span>
<span class="c1"># a uniform way.</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">set_style</span><span class="p">()</span><span class="o">.</span><span class="n">set_general_style_parameters</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Image Example</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="c1"># add axes to the image</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="c1"># read and plot the image</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;BCCD_Dataset/BCCD/JPEGImages/BloodImage_00000.jpg&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x7fa988e054a8&gt;
</pre></div>
</div>
<img alt="../_images/Logistic_regression_with_one_neuron_26_1.png" src="../_images/Logistic_regression_with_one_neuron_26_1.png" />
</div>
</div>
<p>Notice that our features are a simplified version of all the image, in fact for each image we only have 4 values (<code class="docutils literal notranslate"><span class="pre">xmin</span></code>,<code class="docutils literal notranslate"><span class="pre">xmax</span></code>,<code class="docutils literal notranslate"><span class="pre">ymin</span></code> and <code class="docutils literal notranslate"><span class="pre">ymax</span></code>).</p>
</div>
<div class="section" id="dataset-splitting">
<h3>Dataset Splitting<a class="headerlink" href="#dataset-splitting" title="Permalink to this headline">¶</a></h3>
<p><em>In any machine learning project, it is a good behaviour to split the dataset you have at your disposal in different subsets</em>. Plenty of theoretical explanations about this need is present in literature. In the <a class="reference external" href="#fr">Further Readings</a> section of the notebook you will find some advice on useful material about this topic. To simply explain the concept: when you build a machine learning model, you first need to train (i.e. build) the model and then you have to test it (i.e. verify the model’s performances on never seen before data). The roughest way to do this is to split the dataset into two subsets: 80% of the original dataset to train the model (the more data you have the better your model will perform) and the remaining 20% to test it.</p>
<p>Now we build a train and a test set splitting the dataset randomly in two parts with the following proportions: <strong>80%/20%</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">rnd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bccd_features</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.8</span>

<span class="n">train_x</span> <span class="o">=</span> <span class="n">bccd_features</span><span class="p">[</span><span class="n">rnd</span><span class="p">]</span> <span class="c1"># training dataset (features)</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">bccd_labels</span><span class="p">[</span><span class="n">rnd</span><span class="p">]</span> <span class="c1"># training dataset (labels)</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">bccd_features</span><span class="p">[</span><span class="o">~</span><span class="n">rnd</span><span class="p">]</span> <span class="c1"># testing dataset (features)</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">bccd_labels</span><span class="p">[</span><span class="o">~</span><span class="n">rnd</span><span class="p">]</span> <span class="c1"># testing dataset (labels)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The training dataset dimensions are: &#39;</span><span class="p">,</span> <span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The testing dataset dimensions are: &#39;</span><span class="p">,</span> <span class="n">test_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The training dataset dimensions are:  (3631, 4)
The testing dataset dimensions are:  (896, 4)
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="logistic-regression-the-model">
<h3>Logistic Regression: the Model<a class="headerlink" href="#logistic-regression-the-model" title="Permalink to this headline">¶</a></h3>
<p>Logistic Regression is a classic classification algorithm. Our model will be made of <strong>one neuron</strong> and its goal will be to recognize two classes (labeled as 0 or 1, referring to RBC or WBC inside a cell image). This is an example of a <em>binary classification problem</em>.</p>
<p>Differently from Linear Regression, the activation function will be a <strong>sigmoid function</strong> (leading to a different neuron’s output) and the cost function will be the <strong>cross-entropy</strong>. In the case of Linear Regression an identity activation function and a MSE cost function were used.</p>
<p>The formula of the cross-entropy for one observation is the following:</p>
<div class="math notranslate nohighlight">
\[
L(\hat{y}^{(i)},y^{(i)})=-(y^{(i)}\log{\hat{y}^{(i)}}+(1-y^{(i)})\log({1-\hat{y}^{(i)}}))
\]</div>
<p>In th presence of more than one observation. the cost function is the sum over all observations:</p>
<div class="math notranslate nohighlight">
\[
J({\bf w},b)=\frac{1}{m}\sum_{i=1}^{m}{L(\hat{y}^{(i)},y^{(i)})}
\]</div>
<p>If you are interest in the details concerning cross-entropy, you can find additional material in the <a class="reference external" href="#fr">Further Readings</a> section of this notebook.</p>
<p>The neuron will give as <strong>output</strong> the probability of the input to be of class 1: <span class="math notranslate nohighlight">\(P(y=1|x)\)</span>. Then, images will be classified of class 1 if <span class="math notranslate nohighlight">\(P(y=1|x) &gt; 0.5\)</span> or of class 0 if <span class="math notranslate nohighlight">\(P(y=1|x) &lt; 0.5\)</span>. In the Linear Regression example, the neuron’s output was the continuous predicted variable.</p>
<p>Since we want our neuron to output a probability, the activity function will need to assume values between 0 and 1. The formula of the sigmoid function is the following:</p>
<div class="math notranslate nohighlight">
\[
\sigma(z)=\frac{1}{1+e^{-z}}
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The following lines are need to convert the labels from RBC/WBC notation to 0/1 notation.</span>
<span class="c1"># This is fundamental to correctly train the net.</span>

<span class="n">train_y_bin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_y</span><span class="p">))</span>
<span class="n">train_y_bin</span><span class="p">[</span><span class="n">train_y</span> <span class="o">==</span> <span class="s1">&#39;WBC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">test_y_bin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_y</span><span class="p">))</span>
<span class="n">test_y_bin</span><span class="p">[</span><span class="n">test_y</span> <span class="o">==</span> <span class="s1">&#39;WBC&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="structure-of-the-net">
<h3>Structure of the Net<a class="headerlink" href="#structure-of-the-net" title="Permalink to this headline">¶</a></h3>
<p>The following function builds the one neuron model for logistic regression. The implementation is very similar to that of Linear Regression. The differences, as already mentioned, are the activation function, the cost function and the metrics (accuracy in this case, which we will analyze more in detail in the testing phase).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_model</span><span class="p">():</span>

  <span class="c1"># one unit as network&#39;s output</span>
  <span class="c1"># sigmoid function as activation function</span>
  <span class="c1"># sequential groups a linear stack of layers into a tf.keras.Model</span>
  <span class="c1"># activation parameter: if you don&#39;t specify anything, no activation </span>
  <span class="c1"># is applied (i.e. &quot;linear&quot; activation: a(x) = x).</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span> 
    <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">train_x</span><span class="o">.</span><span class="n">columns</span><span class="p">)],</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
  <span class="p">])</span>

  <span class="c1"># optimizer that implements the RMSprop algorithm</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">)</span>

  <span class="c1"># the compile() method takes a metrics argument, which can be a list of metrics</span>
  <span class="c1"># loss = cross-entropy, metrics = accuracy,</span>
  <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
                <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">,</span>
                <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span><span class="s1">&#39;binary_accuracy&#39;</span><span class="p">])</span>
  
  <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s have a look at the model summary:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 1)                 5         
=================================================================
Total params: 5
Trainable params: 5
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p><strong>Learning rate</strong> is a very important parameter of the optimizer. In fact, it strongly influences the convergence of the minimization process. It is a common and good behaviour to try different learning rate values and see how the model’s convergence changes. You can find further reading advices about this topic in the <a class="reference external" href="#fr">Further Readings</a> section of this notebook.</p>
</div>
<div class="section" id="training-phase-model-s-learning-phase">
<h3>Training Phase (Model’s Learning Phase)<a class="headerlink" href="#training-phase-model-s-learning-phase" title="Permalink to this headline">¶</a></h3>
<p>Training our neuron means finding the weights and biases that minimize a chosen function (usually called the <strong>cost function</strong> and typically indicated by <span class="math notranslate nohighlight">\(J\)</span>). The cost function we chose to minimize in our logistic regression task is the <strong>cross-entropy</strong>. The most famous numerical method to find the minimum of a given function is the <strong>gradient descent</strong> (it is suited for cases in which the solution can not be found analytically, such as all neural network applications). In our example we used the RMSprop algorithm as optimizer.</p>
<p>The minimization process is iterative, therefore it is necessary to decide when to stop it. The simplest way is to set a number of repetitions (called <strong>epochs</strong>) and to run the algorithm that fixed number of times. Then, results are checked to see if an optimal point has been reached. If not, the number of epochs is increased.</p>
<p>We start training our model for <strong>500</strong> epochs and we look at the summary in terms of performances (accuracy).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
  <span class="n">train_x</span><span class="p">,</span> <span class="n">train_y_bin</span><span class="p">,</span>
  <span class="n">epochs</span> <span class="o">=</span> <span class="n">EPOCHS</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
  <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">tfdocs</span><span class="o">.</span><span class="n">modeling</span><span class="o">.</span><span class="n">EpochDots</span><span class="p">()])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch: 0, binary_accuracy:0.3451,  binary_crossentropy:37.0837,  loss:37.0837,  
....................................................................................................
Epoch: 100, binary_accuracy:0.9581,  binary_crossentropy:0.1588,  loss:0.1588,  
....................................................................................................
Epoch: 200, binary_accuracy:0.9733,  binary_crossentropy:0.1039,  loss:0.1039,  
....................................................................................................
Epoch: 300, binary_accuracy:0.9780,  binary_crossentropy:0.0808,  loss:0.0808,  
....................................................................................................
Epoch: 400, binary_accuracy:0.9793,  binary_crossentropy:0.0763,  loss:0.0763,  
....................................................................................................
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hist</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">)</span>
<span class="n">hist</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">epoch</span>
<span class="n">hist</span><span class="o">.</span><span class="n">tail</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss</th>
      <th>binary_crossentropy</th>
      <th>binary_accuracy</th>
      <th>epoch</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>495</th>
      <td>0.070310</td>
      <td>0.070310</td>
      <td>0.979069</td>
      <td>495</td>
    </tr>
    <tr>
      <th>496</th>
      <td>0.068867</td>
      <td>0.068867</td>
      <td>0.979069</td>
      <td>496</td>
    </tr>
    <tr>
      <th>497</th>
      <td>0.070882</td>
      <td>0.070882</td>
      <td>0.979895</td>
      <td>497</td>
    </tr>
    <tr>
      <th>498</th>
      <td>0.072805</td>
      <td>0.072805</td>
      <td>0.980446</td>
      <td>498</td>
    </tr>
    <tr>
      <th>499</th>
      <td>0.069172</td>
      <td>0.069172</td>
      <td>0.981272</td>
      <td>499</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>You can noticed that while the number of epochs increases, the MSE is minimized. But <em>which is the best number of epochs to set</em>? A possible hint can be given by the plot of the <strong>cost function vs. number of iterations</strong>. Let’s plot it. If you are interested in plotting details you can find the complete code inside the /module folder.</p>
<p>The cost function vs. number of iterations plot is also useful to evaluate the model’s convergence for different learning rates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The following line contains the path to fonts that are used to plot result in</span>
<span class="c1"># a uniform way.</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">set_style</span><span class="p">()</span><span class="o">.</span><span class="n">set_general_style_parameters</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Cost Function vs. Number of Iterations PLOT</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hist</span><span class="p">[</span><span class="s1">&#39;epoch&#39;</span><span class="p">],</span> <span class="n">hist</span><span class="p">[</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cost Function (cross-entropy)&#39;</span><span class="p">,</span> <span class="n">fontproperties</span> <span class="o">=</span> <span class="n">fm</span><span class="o">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span> <span class="o">=</span> <span class="n">f</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Iterations&#39;</span><span class="p">,</span> <span class="n">fontproperties</span> <span class="o">=</span> <span class="n">fm</span><span class="o">.</span><span class="n">FontProperties</span><span class="p">(</span><span class="n">fname</span> <span class="o">=</span> <span class="n">f</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Logistic_regression_with_one_neuron_49_0.png" src="../_images/Logistic_regression_with_one_neuron_49_0.png" />
</div>
</div>
<p>Looking at the previous plot, you can notice that, after 100 epochs, the cost function remains almost constant in its value, indicating that a minimum has been reached.</p>
</div>
<div class="section" id="testing-phase-model-s-performances-evaluation">
<h3>Testing Phase (Model’s Performances Evaluation)<a class="headerlink" href="#testing-phase-model-s-performances-evaluation" title="Permalink to this headline">¶</a></h3>
<p>Now, to know if the model you have just built is suited to be applied to unseen data, you have to check its performances over the test set. Moreover, an optimizing metric must be chosen. For a binary classification problem, a classic metric is <strong>accuracy</strong> which can be understood as a measure of how well the classifier correctly identified the two classes of the dataset.</p>
<div class="math notranslate nohighlight">
\[
accuracy=\frac{\text{number of cases correctly identified}}{\text{total number of cases}}
\]</div>
<p>where the number of cases correctly identified is the sum of all positive samples and negative samples (i.e. all 0s and 1s) that were correctly classified, usually called <em>true positives</em> and <em>true negatives</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_x</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="c1"># predict cell type with the built logistic regression model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The following lines compute the accuracy on the test set.</span>

<span class="n">test_predictions1</span> <span class="o">=</span> <span class="n">test_predictions</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="n">tp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">test_predictions1</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">test_y_bin</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">tn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">test_predictions1</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">test_y_bin</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
<span class="n">accuracy_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">tp</span> <span class="o">+</span> <span class="n">tn</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">test_y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The accuracy on the test set is equal to: &#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">accuracy_test</span><span class="o">*</span><span class="mi">100</span><span class="p">),</span> <span class="s1">&#39;%.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The accuracy on the test set is equal to:  96 %.
</pre></div>
</div>
</div>
</div>
<p>Notice that we could achieve a very good accuracy using only one neuron.</p>
</div>
</div>
<div class="section" id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>[<em>Medium Difficulty</em>] Try to change the <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> parameter and see how the model’s convergence changes. Then try to reduce the <code class="docutils literal notranslate"><span class="pre">EPOCHS</span></code> parameter and see when the model cannot reach convergence.</p></li>
<li><p>[<em>Medium Difficulty</em>] Try to see how model’s results change based on the training dataset’s size (reduce it and use different sizes comparing the final results).</p></li>
<li><p>[<em>Hard Difficulty</em>] Try to add to labels <code class="docutils literal notranslate"><span class="pre">Platelets</span></code> samples and generalize the binary classification model to a multiclass one (3 possible classes).</p></li>
</ol>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p><a class="reference external" href="https://www.tensorflow.org/datasets/catalog/bccd">https://www.tensorflow.org/datasets/catalog/bccd</a> (dataset explanation)</p></li>
<li><p>Michelucci, Umberto. “Applied Deep Learning.” A Case-Based Approach to Understanding Deep Neural Networks (2018) (mathematical details about how to perform logistic regression with one neuron)</p></li>
</ol>
</div>
<div class="section" id="further-readings-a-name-fr-a">
<h2>Further Readings <a name = 'fr'></a><a class="headerlink" href="#further-readings-a-name-fr-a" title="Permalink to this headline">¶</a></h2>
<p><strong>Dataset Splitting, Overfitting &amp; Underfitting</strong></p>
<ol class="simple">
<li><p>Lever, Jake, Martin Krzywinski, and Naomi Altman. “Points of significance: model selection and overfitting.” (2016): 703.</p></li>
<li><p>Srivastava, Nitish, et al. “Dropout: a simple way to prevent neural networks from overfitting.” The journal of machine learning research 15.1 (2014): 1929-1958.</p></li>
</ol>
<p><strong>Cross-entropy</strong></p>
<ol class="simple">
<li><p><a class="reference external" href="https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/">https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/</a></p></li>
</ol>
<p><strong>Learning Rate</strong></p>
<ol class="simple">
<li><p>Bengio, Yoshua. “Practical recommendations for gradient-based training of deep architectures.” Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 437-478.</p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./single_neuron"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="Linear_regression_with_numpy.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Linear Regression with NumPy</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="../FFNN/Multiclass_classification_with_fully_connected_networks.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Multiclass Classification with Fully Connected Networks</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Umberto Michelucci with code editing by Michela Sperti<br/>
        
            &copy; Copyright TOELT LLC (2020-2022).<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>